{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgPU8RNtmJyI"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load necessary libraries\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load the dataset\n",
        "file_path = 'creditcard.csv'  # Update with your file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "TTZD2fM-mSo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load additional libraries for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Check the distribution of the target variable\n",
        "sns.countplot(x='Class', data=df)\n",
        "plt.title('Distribution of Fraudulent Transactions')\n",
        "plt.xlabel('Class (0: Genuine, 1: Fraud)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Show basic statistics of the dataset\n",
        "print(df.describe())\n"
      ],
      "metadata": {
        "id": "t4YT-oBBmSlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Check for missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Step 4: Load library for scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale the 'Amount' feature\n",
        "scaler = StandardScaler()\n",
        "df['scaled_amount'] = scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\n",
        "\n",
        "# Drop the 'Time' and 'Amount' columns, as we'll use 'scaled_amount'\n",
        "df.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n"
      ],
      "metadata": {
        "id": "a_30FjTPmSjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Load library for handling imbalanced datasets\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Use SMOTE to handle the imbalanced dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Check the new class distribution\n",
        "sns.countplot(x=y_resampled)\n",
        "plt.title('Distribution of Fraudulent Transactions After SMOTE')\n",
        "plt.xlabel('Class (0: Genuine, 1: Fraud)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "clA-OGZOmSge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Load library for train-test splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "3nXact3LmSeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Load library for model training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize and train the logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "LxVSK2EumSbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Load libraries for evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ceT6NUeYmnHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I_XM3eZ3mnEK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
